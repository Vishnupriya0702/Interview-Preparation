# Interview-Preparation

Data Engineering Road Map

Prerequisites 
---------------------
1. Linux commands
2. programming fundamentals
3. SQL is very important

You should learn the below things
--------------------------------------
1. Distributed Computing Fundamentals
2. Data Lake - HDFS/Amazon S3
3. One data ingestion tool - Ex Sqoop
4. Air flow, oozie, zookeeper.
5. DWH concepts - Hive
6. One NOSQL Database - Hbase/cassandra
7. Functional programming - Scala/Python
8. In-memory computation using Apache Spark
9. Structured Streaming with Kafka for real time data
10. One of the Cloud - AWS/Azure/GCP
11. Integration of various components
12. One Scheduling/Monitoring tool - Airflow
13. CICD for production readiness
14. Do a couple of projects to get a good feel of it.
15. End to end Project feel.

Other interview Questions:
1. Difference between ETL & Hadoop
2. How to improve performance in Spark
3. Where we can reduce the partitions.
4. How to improve performance in Hive.
5. Difference between Hive and other datawarehouse tools.
6.Worked as ETL -developer in various tools such as Informatica/Abinitio. Also, have 3+ years of experience in Big Data technologies such as Spark with functional Programming languages(Python, Scala), Hive, Snowflake. Familiar with AWS S3, Redshift, SNS and EMR.

If you are having 8+ years experience then focus on -
Performance Tuning Part & Design Aspects

If you are targeting Top Product based companies then 
Data Structures & Algorithm is also very important.
Arrays, LinkedList & Trees should be good enough.

Remember, don't just learn to prepare for interviews.
your objective should be to effectively work on projects.

So it's important to focus more on internals & this will be the best way to be interview ready also!

If I am missing anything please feel free to add in comments.
